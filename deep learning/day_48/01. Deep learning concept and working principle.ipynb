{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de95196d",
   "metadata": {},
   "source": [
    "## 딥러닝\n",
    "- 다중 처리 계층(multiple processing layers)으로 구성된 계산 모델을 사용하여 여러 수준의 추상화를 통해 데이터 표현을 학습\n",
    "<br>\n",
    "- Machine Learning 분야에서 가장 각광받는 인공지능 기술의 핵심\n",
    "- 인공신경망을 조금 복잡하게 만들어 놓은 모델\n",
    "- 1958년 Rosenblatt이 제안한 최초 인공신경망 모델 : 퍼셉트론\n",
    "- 사람의 뇌와 비슷하게 동작하도록 구성\n",
    "- 일정 크기 이하라면 활성화(activation)되지 않도록 구성\n",
    "\n",
    "#### 1st AI WINTER : XOR 문제 대두\n",
    "- OR과 AND에 대해서는 잘 동작하는데 XOR 문제는 linear 방식으로 문제를 풀 수 없었음\n",
    "- 1969년 MIT AI Lab의 창립자였던 Minskey의 [Perceptron] 저서를 통해 '현재의 퍼셉트론으로는 XOR이 절대 불가능하다' 를 수학적으로 증명\n",
    "- 1st AI Winter : Perceptron 이론의 한계 발견, XOR 적용 안됨 -> 인공지능 외면\n",
    "\n",
    "#### XOR 문제 해결 : Back propagation\n",
    "- 1986년의 논문에서 발표됨\n",
    "- Output layer부터 Input layer까지 반대로 에러를 보정하는 기술\n",
    "\n",
    "#### 2st AI WINTER : Vanishing gradient problem\n",
    "- backprogation에서 결과를 전달할 때 sigmoid를 사용\n",
    "- backplayer를 지날 때마다 최초 값보다 현저하게 작아지기 때문에 값을 전달해도 의미를 가질 수 없었음\n",
    "- 2nd AI Winter : Vanishing gradient problem occurs in deep models\n",
    "    - Input layer에 가까워질수록 기울기가 0에 가까워져서 나중에는 거의 Gradient descent가 일어나지 않아 학습이 진행되지 않음\n",
    "    \n",
    "### ReLu : Rectified Linear Unit\n",
    "Underfitting : 덜하거나(학습이 잘 안됨)\n",
    "- vanishing gradient 문제의 원인 : non-linearity 즉 sigmoid 함수에 문제가 있음을 발견\n",
    "- ReLu 함수 제안 : 사그라드는 sigmoid 대신 죽지 않는 activation function\n",
    "    - 0과 현재 값(x) 중에서 큰 값을 선택 ->> 코드로 구현하면 max(0,x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0a9470",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "\n",
    "## Stochastic Gradient descent(SGD : 확률 그라디언트 하강)\n",
    "#### Gradient Decent\n",
    "- 기존 뉴럴넷이 가중치 parameter들을 최적화 하는 방법\n",
    "- loss function의 현 가중치에서 기울기를 구해서 loss를 줄이는 방법으로 업데이트 해감\n",
    "#### SGD\n",
    "- 일부의 데이터로 mini batch를 형성하여 한 batch에 대한 gradient만을 계산하여 전체 parameter을 업데이트\n",
    "\n",
    "#### Recent Revival\n",
    "1. Computing Power(특히 GPU 기반 경렬 계산)\n",
    "2. Big Data\n",
    "3. 획기적인 알고리즘\n",
    "4. 개발 환경의 진화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ab0ccb",
   "metadata": {},
   "source": [
    "***\n",
    "***\n",
    "## Supervised Learning(감독 학습)\n",
    "- 알파고 : 기존의 기보를 학슴-> Supervised Learnig-> training data set\n",
    "- Google news grouping -> Unsupervised Learning\n",
    "\n",
    "#### Multilayer neural network\n",
    "- 입력 공간을 왜곡시켜 데이터 클래스는 선형으로 분리 가능\n",
    "- 데이터에 근거해서 선을 긋고 구분 결과가 좋게 나오도록 최적화하는 과정을 거침\n",
    "\n",
    "### CNN(Convolution Neural Networks)\n",
    "- 사물을 볼 때 뇌 전체가 아닌 일부만 활성화 됨을 확인\n",
    "- 컬러 이미지를 위한 딥 컨볼루션 신경망 작동 확인\n",
    "- 패턴들을 쌓아가며 점차 복잡한 패턴을 인식 (CONV)\n",
    "- 사이즈를 줄여가며 추상화\n",
    "\n",
    "#### Max Pooling\n",
    "- Pooling -> sampling 또는 resizing\n",
    "- 단순하게 존재하는 값 중에서 하나를 선택, 선명한 정보만 남김\n",
    "- cnn에서는 가장 큰 값을 선택하는 max pooling을 주요 사용\n",
    "\n",
    "#### Drop out\n",
    "Overfitting(과하거나): 문제 해결을 위해 융통성을 갖자\n",
    "- 뉴럴 네트워크를 끊어서 몇개의 노드를 줄임\n",
    "- training시 몇 개의 뉴런을 쉬게 하고, 나머지만 가지고 훈련.. 총동원해서 예측\n",
    "\n",
    "#### Distributed representations ans lantuage processing\n",
    "- 사진의 상황을 묘사하는 기술, 입력 이미지에 대한 순환망에 의해 자동 생성된 캡션\n",
    "- 이미지를 CNN 처리한 후 RNN을 통해 캡션 문장을 생성\n",
    "- 언어 처리 분야 : 딥러닝 기술의 도입으로 획기적 발전\n",
    "- 단어 임베딩 : 단어를 고차원 연속 공간에 할당. 의미가 유사하면 거리도 가깝도록 임베딩\n",
    "- 단어 의미의 유사성은 문장 안에서 인접한 단어이ㅡ 분포가 얼마나 유사한지를 기준으로 판단\n",
    "\n",
    "### RNN(Recurrent Neural Network)\n",
    "- 새로운 데이터 입력이 DL의 Hidden Unit 조정\n",
    "- 새로 배운것에 따라 아웃풋이 바뀜\n",
    "- DL이 자신의 경험을 기억하도록 만드는 방식이라 볼 수 있음\n",
    "- RNN은 일반적인 DL 보다 언어, 사람의 문장을 이해하는 데 더 뛰어나서 최근 주목을 받고 있음\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f46bd6",
   "metadata": {},
   "source": [
    "### The Future of Deep Learning\n",
    "#### 비감독(Unsupervised learning)\n",
    "- 비감독 이 감독학습보다 장기적인 측면에서 더 중요\n",
    "- 사람과 동물의 학습은 비감독 학습\n",
    "#### Computer vision\n",
    "- CNN과 강화학습을 이용한 리커런트 뉴럴 네트워크 RNNs의 결합 시스템이 크게 발전\n",
    "#### Natural Language Understanding\n",
    "- 히든 유닛에서 이전 시퀀스의 엘리먼트를 기억하면서 한번에 하나의 엘리먼트를 시퀀스 입력으로 처리\n",
    "#### 복잡한 추론을 통한 표현학습 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b4eba3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
