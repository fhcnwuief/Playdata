{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c602884",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q KoNLPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e781aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from konlpy.tag import Okt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Bidirectional, Dense, LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88e18080",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_TRAIN_PATH = tf.keras.utils.get_file(\"shakespeare.txt\",\n",
    "                         'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95e41a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max.colwidth', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c131061a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32777, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>First Citizen:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Before we proceed any further, hear me speak.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>All:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Speak, speak.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>First Citizen:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32772</th>\n",
       "      <td>And yet so fast asleep.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32773</th>\n",
       "      <td>ANTONIO:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32774</th>\n",
       "      <td>Noble Sebastian,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32775</th>\n",
       "      <td>Thou let'st thy fortune sleep--die, rather; wink'st</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32776</th>\n",
       "      <td>Whiles thou art waking.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>32777 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                         0\n",
       "0                                           First Citizen:\n",
       "1            Before we proceed any further, hear me speak.\n",
       "2                                                     All:\n",
       "3                                            Speak, speak.\n",
       "4                                           First Citizen:\n",
       "...                                                    ...\n",
       "32772                              And yet so fast asleep.\n",
       "32773                                             ANTONIO:\n",
       "32774                                     Noble Sebastian,\n",
       "32775  Thou let'st thy fortune sleep--die, rather; wink'st\n",
       "32776                              Whiles thou art waking.\n",
       "\n",
       "[32777 rows x 1 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data = pd.read_csv('./storage.googleapis.com_download.tensorflow.org_data_shakespeare.txt', delimiter='\\t',header=None)\n",
    "print(train_data.shape)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2adf264",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04635f7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d0bd6ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'All:'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = str(train_data[0].loc[2])\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31a1df68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32777"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2e1c62d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All:\n"
     ]
    }
   ],
   "source": [
    "llist = []\n",
    "for num in range(train_data.shape[0]):\n",
    "    llist.append(str(train_data[0].loc[num]))\n",
    "print(llist[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c7108289",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['All', ':']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "okt = Okt()\n",
    "test = str(test)\n",
    "okt.morphs(test, stem=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7deb5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LEEMIJU\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb0e7515",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "020f55a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b638434",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Before', 'proceed', 'hear', 'speak']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords \n",
    "from nltk.tokenize import word_tokenize \n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words.append(',')\n",
    "stop_words.append('.')\n",
    "stop_words.append(':')\n",
    "stop_words.append('...')\n",
    "result = []\n",
    "finlist = []\n",
    "\n",
    "for num in range(len(llist)):\n",
    "    word_tokens = word_tokenize(llist[num])\n",
    "\n",
    "    result = []\n",
    "    for w in word_tokens: \n",
    "        if w not in stop_words: \n",
    "            result.append(w)\n",
    "        finlist.append(result)\n",
    "        \n",
    "\n",
    "print(finlist[10]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "619f862a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def preprocessing(sentence, remove_stopwords=True):\n",
    "    # 불용어 제거\n",
    "    #stop_words = set(['에', '은', '는', '이', '가', '그리고', '것', '들', '수', '등', '로', '을', '를', '만', '도', '아', '의', '그', '다'])\n",
    "    stop_words = []\n",
    "\n",
    "    sentence = re.sub(r'[^a-zA-Z ]', '', sentence)  # 영어 문자와 공백을 제외한 모든 문자 제거\n",
    "    sentence = okt.morphs(sentence, stem=True)\n",
    "    if remove_stopwords:\n",
    "        sentence = [token for token in sentence if not token in stop_words]\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c9055ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train processed = 0\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'tuple'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m<timed exec>:8\u001b[0m\n",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m, in \u001b[0;36mpreprocessing\u001b[1;34m(sentence, remove_stopwords)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpreprocessing\u001b[39m(sentence, remove_stopwords\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# 불용어 제거\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#stop_words = set(['에', '은', '는', '이', '가', '그리고', '것', '들', '수', '등', '로', '을', '를', '만', '도', '아', '의', '그', '다'])\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     stop_words \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 8\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[^a-zA-Z ]\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, sentence)  \u001b[38;5;66;03m# 영어 문자와 공백을 제외한 모든 문자 제거\u001b[39;00m\n\u001b[0;32m      9\u001b[0m     sentence \u001b[38;5;241m=\u001b[39m okt\u001b[38;5;241m.\u001b[39mmorphs(sentence, stem\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m remove_stopwords:\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\re\\__init__.py:185\u001b[0m, in \u001b[0;36msub\u001b[1;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[0;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msub\u001b[39m(pattern, repl, string, count\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    179\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return the string obtained by replacing the leftmost\u001b[39;00m\n\u001b[0;32m    180\u001b[0m \u001b[38;5;124;03m    non-overlapping occurrences of the pattern in string by the\u001b[39;00m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;124;03m    replacement repl.  repl can be either a string or a callable;\u001b[39;00m\n\u001b[0;32m    182\u001b[0m \u001b[38;5;124;03m    if a string, backslash escapes in it are processed.  If it is\u001b[39;00m\n\u001b[0;32m    183\u001b[0m \u001b[38;5;124;03m    a callable, it's passed the Match object and must return\u001b[39;00m\n\u001b[0;32m    184\u001b[0m \u001b[38;5;124;03m    a replacement string to be used.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 185\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msub(repl, string, count)\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object, got 'tuple'"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_sentences = []\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "for i, sent in enumerate(zip(train_data[0])):\n",
    "    if i % 10000 == 0:\n",
    "        print(f\"Train processed = {i}\")\n",
    "    sent = preprocessing(sent)\n",
    "    if len(sent) > 0:\n",
    "        train_sentences.append(sent)\n",
    "        train_labels.append(label)\n",
    "\n",
    "print(time.time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0d62c65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32777,)\n"
     ]
    }
   ],
   "source": [
    "train_labels = np.array(llist)\n",
    "\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "940b564e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentences' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m VOCAB_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m20000\u001b[39m\n\u001b[0;32m      3\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m Tokenizer(num_words\u001b[38;5;241m=\u001b[39mVOCAB_SIZE, oov_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<OOV>\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mfit_on_texts(train_sentences)\n\u001b[0;32m      6\u001b[0m train_sequences \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtexts_to_sequences(train_sentences)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_sequences[\u001b[38;5;241m0\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_sentences' is not defined"
     ]
    }
   ],
   "source": [
    "VOCAB_SIZE = 20000\n",
    "\n",
    "tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(train_sentences)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(train_sentences)\n",
    "\n",
    "print(train_sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c596f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gtts\n",
      "  Downloading gTTS-2.3.2-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from gtts) (2.29.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from gtts) (8.0.4)\n",
      "Requirement already satisfied: colorama in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\leemiju\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2023.5.7)\n",
      "Installing collected packages: gtts\n",
      "Successfully installed gtts-2.3.2\n"
     ]
    }
   ],
   "source": [
    "!pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6b728f67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "\n",
    "# TTS로 변환할 텍스트\n",
    "text = \"안녕하세요, 반갑습니다.\"\n",
    "\n",
    "# gTTS 객체 생성\n",
    "tts = gTTS(text, lang='ko')  # 'ko'는 한국어를 나타냅니다\n",
    "\n",
    "# 음성 파일 저장\n",
    "tts.save(\"output.mp3\")\n",
    "\n",
    "# 생성된 음성 파일 재생 (운영체제에 따라 다를 수 있음)\n",
    "os.system(\"start output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06cbece0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
